# Data Engineering Project Portfolio

This repository highlights a collection of my data engineering projects. Each project showcases hands-on experience with modern data tools, cloud platforms, and real-world use cases in building scalable, automated data pipelines and analytics solutions.

---

## 1. Olympic Data Analytics (End-to-End Azure Project)

**GitHub Repository:** [Olympic_Data_Analytics](https://github.com/SanjeevikumarWD/Olympic_Data_Analytics)

**Overview:**  
A complete Azure-based data engineering solution to analyze Tokyo Olympics data. The project implements an enterprise-grade architecture covering ingestion, transformation, storage, and visualization using Azure tools.

**Key Features:**

- Ingested raw CSV data from GitHub using Azure Data Factory  
- Stored data in Azure Data Lake Gen2 (Raw & Transformed Zones)  
- Transformed and cleaned data using Azure Databricks (PySpark)  
- Analyzed transformed data using Azure Synapse Analytics (SQL pool)  
- Created dashboards using built-in Synapse Studio charts  

**Technology Stack:**  
Azure Data Factory, Azure Data Lake Gen2, Azure Databricks, Azure Synapse Analytics, Synapse Studio, GitHub

---

## 2. Customer Transaction ETL Pipeline

**GitHub Repository:** [Customer-Transaction-ETL-Pipeline](https://github.com/SanjeevikumarWD/Customer-Transaction-ETL-Pipeline)

**Overview:**  
Simulates a banking analytics ETL workflow using PySpark, PostgreSQL, and Airflow to process and analyze synthetic customer transaction data.

**Key Features:**

- Extracted CSV data using PySpark  
- Cleaned nulls and filtered invalid transactions  
- Aggregated total spend by customer  
- Loaded results into a PostgreSQL database  
- Automated the pipeline using Apache Airflow DAGs  
- Dockerized environment for portability  

**Technology Stack:**  
PySpark, PostgreSQL, Apache Airflow, Docker

---

## 3. Stock Price Data Warehouse

**GitHub Repository:** [Stock_Price_Data_Warehouse](https://github.com/SanjeevikumarWD/stock_price_data_warehouse)

**Overview:**  
Builds a modular data warehouse for financial analytics using PySpark, DBT, PostgreSQL, and Airflow. Designed for efficient batch processing and SQL-based modeling.

**Key Features:**

- Extracted stock price data using PySpark  
- Cleaned and transformed data  
- Performed modeling and aggregation using DBT  
- Loaded data into PostgreSQL warehouse  
- Scheduled and monitored workflows using Apache Airflow  
- Built with containerized infrastructure  

**Technology Stack:**  
PySpark, PostgreSQL, DBT, Apache Airflow, Docker

---

## Contact

**Name:** Sanjeevikumar M  
**Email:** sanjeevikumar585@gmail.com  
**LinkedIn:** [https://www.linkedin.com/in/sanjeevikumar12/](https://www.linkedin.com/in/sanjeevikumar12/)

---

*This repository is part of my data engineering portfolio and reflects my commitment to building scalable, efficient, and maintainable data systems. Open to internship and entry-level opportunities.*
